{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e474b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "print(\"torch\", torch.__version__)\n",
    "device = \"cpu\"\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e274885d",
   "metadata": {},
   "source": [
    "## 1) Tensors vs NumPy arrays (conceptual)\n",
    "\n",
    "Important differences:\n",
    "\n",
    "- tensors can live on GPU (`cuda`) and track gradients (`requires_grad=True`)\n",
    "- ops build a computation graph (for differentiable tensors)\n",
    "- many ops are similar to NumPy but not identical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], device=device)\n",
    "w = torch.randn(3, device=device, requires_grad=True)\n",
    "\n",
    "y = (w * x).sum()\n",
    "y.backward()\n",
    "print(\"y\", y.item())\n",
    "print(\"grad w\", w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd803f2c",
   "metadata": {},
   "source": [
    "### Common gotcha: gradients accumulate\n",
    "\n",
    "Calling `.backward()` adds into `.grad`. You typically zero gradients each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad.zero_()\n",
    "y2 = (w * x).sum()\n",
    "y2.backward()\n",
    "print(\"grad after fresh backward\", w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a2f81",
   "metadata": {},
   "source": [
    "## 2) From math to code: linear regression\n",
    "\n",
    "We’ll fit a model $at{y} = Xw + b$ by minimizing mean squared error (MSE).\n",
    "\n",
    "Key idea: autograd gives gradients, and gradient descent updates parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5840f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "n, d = 200, 3\n",
    "X = torch.randn(n, d, device=device)\n",
    "w_true = torch.tensor([2.0, -1.0, 0.5], device=device)\n",
    "b_true = torch.tensor(0.7, device=device)\n",
    "noise = 0.1 * torch.randn(n, device=device)\n",
    "y = X @ w_true + b_true + noise\n",
    "\n",
    "# Parameters to learn\n",
    "w = torch.zeros(d, device=device, requires_grad=True)\n",
    "b = torch.zeros((), device=device, requires_grad=True)\n",
    "\n",
    "\n",
    "def mse(yhat, y):\n",
    "    return ((yhat - y) ** 2).mean()\n",
    "\n",
    "\n",
    "lr = 0.1\n",
    "for step in range(200):\n",
    "    yhat = X @ w + b\n",
    "    loss = mse(yhat, y)\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(step, float(loss))\n",
    "\n",
    "print(\"w learned\", w.detach().cpu().numpy())\n",
    "print(\"b learned\", float(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14f475",
   "metadata": {},
   "source": [
    "## 3) `nn.Module` version (still minimal)\n",
    "\n",
    "This is the same model, but packaged the PyTorch way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131839d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Linear(d, 1, bias=True).to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for step in range(200):\n",
    "    yhat = model(X).squeeze(-1)\n",
    "    loss = ((yhat - y) ** 2).mean()\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(step, float(loss))\n",
    "\n",
    "w_learned = model.weight.detach().squeeze(0)\n",
    "b_learned = model.bias.detach().squeeze(0)\n",
    "print(\"w learned\", w_learned.cpu().numpy())\n",
    "print(\"b learned\", float(b_learned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b47899",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Exercise A — Manual gradient check\n",
    "\n",
    "For linear regression with MSE, derive gradients w.r.t. `w` and `b` and compare to autograd on a single batch.\n",
    "\n",
    "## Exercise B — Logistic regression\n",
    "\n",
    "Generate a 2D synthetic classification dataset and train logistic regression with `binary_cross_entropy_with_logits`.\n",
    "\n",
    "## Exercise C — L2 regularization\n",
    "\n",
    "Add $ambda w^2$ to the loss and observe how weights shrink as $ambda$ increases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter for Exercise B (logistic regression)\n",
    "n = 400\n",
    "X = torch.randn(n, 2, device=device)\n",
    "true_w = torch.tensor([1.5, -2.0], device=device)\n",
    "true_b = torch.tensor(-0.25, device=device)\n",
    "logits = X @ true_w + true_b\n",
    "y = (logits > 0).float()\n",
    "\n",
    "model = nn.Linear(2, 1).to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "\n",
    "for step in range(200):\n",
    "    logits = model(X).squeeze(-1)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(logits, y)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            pred = (logits > 0).float()\n",
    "            acc = (pred == y).float().mean()\n",
    "        print(step, float(loss), float(acc))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
